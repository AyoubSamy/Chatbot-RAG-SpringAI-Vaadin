vaadin.launch-browser=true
spring.application.name=projet_professionnel_chatbot_essaie
#specification de module a utiliser dans notre cas llama3
spring.ai.ollama.chat.model=llama3
#specification de ou ollam est executer  c-a-d ou le serveur de model ollama dans notre cas  c'est local
spring.ai.ollama.base-url=http://localhost:11434

server.port=8080

spring.docker.compose.enabled=false


